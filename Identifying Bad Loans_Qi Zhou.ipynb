{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf839515-70b9-4754-a82e-615e92554b70",
   "metadata": {},
   "source": [
    "#  Identifying Bad Loans"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "17eca114-1a69-4689-aa24-c195a9d8cb6d",
   "metadata": {},
   "source": [
    "#### Objective\n",
    "To identify loans at high risk of becoming non-performing — specifically those that have reached the 2nd delinquency period (i.e. Late for 31 - 120 days), Default, or Charged Off — by analyzing borrower characteristics.\n",
    "\n",
    "#### Metrics\n",
    "Providing a loan to a defaulter results in direct financial loss. Assuming that lenders are risk averse and are more willing to miss a good borrower than fund a bad one. Hence, I seek to minimise the number of defaulters incorrectly classified as good. \n",
    "\n",
    "#### Goal\n",
    "To construct a model that achieves a recall of ≥70%. F1 score should be at least 60% so that a balance of precision and recall is achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eba8e0-a8df-4519-b100-74d54ce87da5",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a46cc2f-29cd-4eed-a216-362d3910ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries for Exploratory Data Analysis\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "# Import libraries for visualisation\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcecd4bd-06e5-4e1b-8cbb-1d12aa898c70",
   "metadata": {},
   "source": [
    "## Importing the data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4ecb24-fd22-45f6-8ff6-88e3736c614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df = pd.read_csv('./Credit Risk_Lending Club/Data_Lending Club_Jun 2007_to_Sep 2020.csv',\n",
    "                                low_memory=False)\n",
    "\n",
    "print(loan_df.shape)\n",
    "loan_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af97c82-f18b-429b-83ad-11feee657c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dictionary = pd.read_csv('./Credit Risk_Lending Club/Data_Dictionary_Lending Club_Jun 2007_to_Sep 2020.csv',\n",
    "                          usecols=['Column','Description'],index_col=0)\n",
    "data_dictionary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d0fa8-c717-44d7-8f37-6752186b8ef3",
   "metadata": {},
   "source": [
    "## Building Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f7350c-a7e3-43d3-be4f-91aab6a9af61",
   "metadata": {},
   "source": [
    "#### Defining a Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a720ebd2-0325-4115-88d6-07058aa51f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_col(col_name):\n",
    "    print(data_dictionary.loc[col_name]['Description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a86d8e-22c0-4461-9f8b-5dd37ad6394f",
   "metadata": {},
   "source": [
    "#### Selecting a Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d745412-930c-4abc-949d-ae88bec87380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cont_stat_sig_test(df,col):\n",
    "    # defining function for normality + statistical significance check\n",
    "    # Step 1: normality test\n",
    "    stat, p_value = stats.normaltest(df[col].values)\n",
    "    \n",
    "    # Step 2: filter series    \n",
    "    good_loan = loan_individual_filled_df[loan_individual_filled_df['updated_loan_status']=='Good Loan'][col].values\n",
    "    bad_loan = loan_individual_filled_df[loan_individual_filled_df['updated_loan_status']=='Bad Loan'][col].values\n",
    "    \n",
    "    # Step 3: determining appropriate statistical significance test based on normality\n",
    "    if p_value>0.01:\n",
    "        distribution = 'normal distribution'\n",
    "        test = 'Anova test'\n",
    "        stat, p_value = stats.f_oneway(good_loan, bad_loan)\n",
    "    \n",
    "    else:\n",
    "        distribution = 'non-normal distribution'\n",
    "        test = 'KS test'\n",
    "        stat, p_value = stats.ks_2samp(good_loan, bad_loan)\n",
    "    \n",
    "    # Step 4: conclude on statistical significance\n",
    "    if p_value>0.05:\n",
    "        sig = 'not statistically significant'\n",
    "    else:\n",
    "        sig = 'statistically significant'\n",
    "\n",
    "    print(f'{col} has {distribution}: running {test}. result: {sig}')\n",
    "    return p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd738c3-2029-45f6-b235-9563319d2f8d",
   "metadata": {},
   "source": [
    "## Creating the SQLite database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892caba-a6ad-423d-a8f0-0f5382148da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import SQLite3 to extract the data set\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df133c65-a134-4433-be8d-b6b8accbbb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new SQlite database\n",
    "con = sqlite3.connect('loan_df.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac25d5b0-9564-449b-99c3-7f78e3a7d9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data set to SQLite database the df in the test_db db\n",
    "loan_df.to_sql(name='loan_df',con=con, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a437a6ea-2ca8-4bc1-8318-87ada7aefd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the first five rows of all columns\n",
    "sql_query_first_5_row = '''\n",
    "SELECT * \n",
    "FROM loan_df\n",
    "LIMIT 5\n",
    "'''\n",
    "\n",
    "sql_query_first_5_row = pd.read_sql(sql_query_first_5_row,con)\n",
    "\n",
    "sql_query_first_5_row"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b814832-0a15-4882-a35e-d1c959e3253d",
   "metadata": {},
   "source": [
    "## Feature Engineering and Preliminary EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8905ad0d-f4f2-4637-9dd2-4e4301a2d01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the columns, the number of null values, and data type\n",
    "\n",
    "loan_df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0559f38e-8acc-4d7c-a07f-fdaa0c160224",
   "metadata": {},
   "source": [
    "### Scoping the Data Set to Individual Loans (i.e. remove non-individual loan applications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba746dbf-43bb-402e-ae53-f289fc73a4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df['application_type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1b9b2f-ce44-4366-9eee-d3ced9979a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_individual_df = loan_df[loan_df['application_type'] == 'Individual'].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf290e2e-0d1a-445b-99e5-49b0a7438db0",
   "metadata": {},
   "source": [
    "### Identifying % of NaN in each Column and sorting them in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794bfa5-f337-4502-9e37-7dc93147b3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking columns with missing values\n",
    "missing_values_df = pd.DataFrame(loan_individual_df.isna().sum()).reset_index()\n",
    "missing_values_df.columns = ['col', 'num_missing']\n",
    "missing_values_df['pct_missing'] = missing_values_df['num_missing']*100/len(loan_df)\n",
    "missing_values_df[missing_values_df['num_missing']>0].round(3)\n",
    "\n",
    "missing_values_descending = (\n",
    "    missing_values_df[missing_values_df['num_missing'] > 0]\n",
    "    .sort_values('pct_missing', ascending=False)\n",
    "    .round(5)\n",
    ")\n",
    "\n",
    "# Count the number of such columns\n",
    "num_cols_with_missing = missing_values_descending.shape[0]\n",
    "print(f\"Number of columns with missing values: {num_cols_with_missing}\")\n",
    "\n",
    "missing_values_descending"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff0c424-2e44-4774-97d1-a01e9d3cf1ef",
   "metadata": {},
   "source": [
    "### Dropping columns that are >10% NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a5b17-8862-492d-b048-2bfb2810c129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify columns with >10% missing\n",
    "NaN_threshold = 10.0\n",
    "missing_10 = missing_values_descending[missing_values_descending['pct_missing'] > NaN_threshold]\n",
    "\n",
    "cols_to_drop = missing_10['col'].tolist()\n",
    "no_cols_to_drop = len(cols_to_drop)\n",
    "print(f\"Number of columns with at least 10% NaN: {no_cols_to_drop}\")\n",
    "\n",
    "loan_individual_90_df = loan_individual_df.drop(columns=cols_to_drop)\n",
    "\n",
    "missing_10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3920d69-f69b-4e52-844e-57ae2dd8ab68",
   "metadata": {},
   "source": [
    "#### Dropping Columns irrelevant to Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc78b4c-ad9b-42ac-92b6-df577dccce99",
   "metadata": {},
   "outputs": [],
   "source": [
    "irrelevant_cols = ['Unnamed: 0','id', \n",
    "                   'funded_amnt', 'funded_amnt_inv',  # alternatives to 'loan_amnt'\n",
    "                   'emp_title', #not standardised titles\n",
    "                   'pymnt_plan', # contains only 1 value i.e. 'n'\n",
    "                   'url', # url of lending_club website\n",
    "                   'zip_code', # masked value\n",
    "                   'title', # alternative to purpose\n",
    "                   'policy_code', # contains only 1 value i.e. '1'\n",
    "                   'initial_list_status', #initial listing status (whole vs. fractional).There are do not appear to be any meaningful differences between whole and fractional loans.\n",
    "                   'total_rev_hi_lim', # 'na' in data_dictionary\n",
    "                   'application_type' # data set scoped to 'application_type' == 'Individual'\n",
    "                  ]\n",
    "\n",
    "loan_individual_90_df = loan_individual_90_df.drop(irrelevant_cols, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e7b9a-4348-438a-82cc-6457d5f946ae",
   "metadata": {},
   "source": [
    "### Dropping all rows with NaN values for remaining columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6ecf25-c238-4bd3-bcf3-69bbf4723fa4",
   "metadata": {},
   "source": [
    "# RESTART HERE FOR loan_individual_filled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbd9fef-0350-4081-8b8e-c6db02da46b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_90 = missing_values_descending[missing_values_descending['pct_missing'] < NaN_threshold]\n",
    "\n",
    "cols_to_drop = filled_90['col'].tolist()\n",
    "no_cols_to_drop = len(cols_to_drop)\n",
    "print(f\"Number of columns with at least 1 row with NaN: {no_cols_to_drop}\")\n",
    "\n",
    "# Drop all rows with NaN values\n",
    "loan_individual_filled_df = loan_individual_90_df.dropna()\n",
    "\n",
    "filled_90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6ac8e3-9b46-4c44-8f12-ea08279ca77b",
   "metadata": {},
   "source": [
    "### Print the shape before and after dropping columns >10% and rows with NaNs for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35d784e-d709-4e7c-a2bb-f1824d801d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original shape of data set: {loan_df.shape}\")\n",
    "print(f\"Shape of data set with columns of individual loans: {loan_individual_df.shape}\")\n",
    "print(f\"Shape of data set with columns of individual loans with >90% filled rows: {loan_individual_90_df.shape}\")\n",
    "print(f\"Shape of data set of individual loans with columns fully filled: {loan_individual_filled_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3269ef60-34cb-4779-8328-38b858de302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_individual_filled_df = loan_individual_filled_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471457fe-e14f-4e26-ac9f-4ce1f74bed54",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_individual_filled_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39ac4e9-8b31-4946-a337-b2c65ae08bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_individual_filled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f1cf8d-6730-42cb-a321-587ed2ad6672",
   "metadata": {},
   "source": [
    "### Formatting the Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efddcf7-4149-4419-bb6a-bd00927e3d13",
   "metadata": {},
   "source": [
    "#### Converting to DateTime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8c206b-e02b-4dca-91e7-652297cda943",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_individual_filled_df[\"issue_d\"] = pd.to_datetime(loan_individual_filled_df[\"issue_d\"], format=\"%b-%Y\", errors='coerce')\n",
    "loan_individual_filled_df[\"earliest_cr_line\"] = pd.to_datetime(loan_individual_filled_df[\"earliest_cr_line\"], format=\"%b-%Y\", errors='coerce')\n",
    "loan_individual_filled_df[\"last_pymnt_d\"] = pd.to_datetime(loan_individual_filled_df[\"last_pymnt_d\"], format=\"%b-%Y\", errors='coerce')\n",
    "loan_individual_filled_df[\"last_credit_pull_d\"] = pd.to_datetime(loan_individual_filled_df[\"last_credit_pull_d\"], format=\"%b-%Y\", errors='coerce')\n",
    "\n",
    "print(\"Data type of 'issue_d' after conversion:\", loan_individual_filled_df['issue_d'].dtype)\n",
    "print(\"Data type of 'earliest_cr_line' after conversion:\", loan_individual_filled_df['earliest_cr_line'].dtype)\n",
    "print(\"Data type of 'last_pymnt_d' after conversion:\", loan_individual_filled_df['last_pymnt_d'].dtype)\n",
    "print(\"Data type of 'last_credit_pull_d' after conversion:\", loan_individual_filled_df['last_credit_pull_d'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bd69d0-2a03-4d63-af97-790007ec8917",
   "metadata": {},
   "source": [
    "#### Converting to Float or Integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e194542d-86ed-498a-b660-53af8a6bd1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'int_rate', 'bc_util', 'revol_util' to float\n",
    "print(\"Data type of 'term' before conversion:\", loan_individual_filled_df['term'].dtype)\n",
    "print(\"Data type of 'int_rate' before conversion:\", loan_individual_filled_df['int_rate'].dtype)\n",
    "print(\"Data type of 'revol_util' before conversion:\", loan_individual_filled_df['revol_util'].dtype)\n",
    "\n",
    "\n",
    "# If the interest rate is stored as a string with '%' (e.g., \"13.56%\"), \n",
    "# remove the '%' and convert the result into a float\n",
    "loan_individual_filled_df['term'] = loan_individual_filled_df['term'].str.extract(r'(\\d+)').astype(int)\n",
    "loan_individual_filled_df['int_rate'] = loan_individual_filled_df['int_rate'].str.rstrip('%').astype(float)\n",
    "loan_individual_filled_df['revol_util'] = loan_individual_filled_df['revol_util'].str.rstrip('%').astype(float)\n",
    "\n",
    "\n",
    "# Verify the data type after conversion\n",
    "print(\"Data type of 'term' after conversion:\", loan_individual_filled_df['term'].dtype)\n",
    "print(\"Data type of 'int_rate' after conversion:\", loan_individual_filled_df['int_rate'].dtype)\n",
    "print(\"Data type of 'revol_util' after conversion:\", loan_individual_filled_df['revol_util'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06052d8b-c7b7-47d3-a941-63e73bee8966",
   "metadata": {},
   "source": [
    "### Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d099daa3-28c2-4107-87fd-6b236ae8dd4a",
   "metadata": {},
   "source": [
    "#### Function to convert 'emp_length' to 'emp_length_int'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196f9436-b3e8-466b-bd79-f65e097d2550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emp_length(emp_str):\n",
    "    emp_str = emp_str.strip()\n",
    "    # Recode \"< 1 year\" as 0\n",
    "    if emp_str == \"< 1 year\":\n",
    "        return 0\n",
    "    # Recode \"10+ years\" as 10\n",
    "    if emp_str == \"10+ years\":\n",
    "        return 10\n",
    "    # Otherwise, extract the numeric portion (e.g., \"3 years\" -> 3)\n",
    "    try:\n",
    "        return int(emp_str.split()[0])\n",
    "    except Exception:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f381fc-3c1c-454e-af75-8c0c0b1db054",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the conversion function to the 'emp_length' column\n",
    "loan_individual_filled_df['emp_length_int'] = loan_individual_filled_df['emp_length'].apply(convert_emp_length)\n",
    "\n",
    "loan_individual_filled_df.drop(['emp_length'], axis=1,inplace=True) # to drop 'emp_length' after conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5ea184-21f5-4323-9836-cdf858878d8f",
   "metadata": {},
   "source": [
    "#### Function to assign the category of 'Bad Loan' and 'Good Loan' to loan_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd4c5ef-1d7a-4e39-ba7d-9f2147bb13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorise_status(status):\n",
    "    status = str(status).strip()\n",
    "    if status == \"Charged Off\":\n",
    "        return \"Bad Loan\"\n",
    "    elif status == \"Does not meet the credit policy. Status:Charged Off\":\n",
    "        return \"Bad Loan\"\n",
    "    elif status == \"Default\":\n",
    "        return \"Bad Loan\"\n",
    "    elif status == \"Late (31-120 days)\":\n",
    "        return \"Bad Loan\"\n",
    "    else:\n",
    "        return \"Good Loan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53650161-5aa4-4973-8eb3-c80c65d23fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the conversion function to the 'loan_status' column\n",
    "loan_individual_filled_df['updated_loan_status'] = loan_individual_filled_df['loan_status'].apply(categorise_status)\n",
    "\n",
    "loan_individual_filled_df.drop(['loan_status'], axis=1,inplace=True) # to drop 'loan_status' after conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e148e037-8434-42eb-bd8c-bcb569cfa20e",
   "metadata": {},
   "source": [
    "#### Target Variable (i.e. Bad Loans) as '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23118078-d41f-4b2d-9918-9b9762845413",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_individual_filled_df['bad_loan'] = loan_individual_filled_df['updated_loan_status'].apply(lambda x: 0 if x == \"Good Loan\" else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1a4f4c-11c8-4fbc-a2d3-51a7f53eb13c",
   "metadata": {},
   "source": [
    "#### 'verification_status' to 'cleaned_verification_status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9616e1cf-284b-4ba5-b9d5-b5f24ea0d604",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_individual_filled_df['verification_status'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f095bc78-50d2-4878-b6b3-e02392cb1ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define 'verified' values\n",
    "verified_values = ['Verified', 'Source Verified']\n",
    "\n",
    "# Create a new 'cleaned_verification' column\n",
    "loan_individual_filled_df['cleaned_verification_status'] = np.where(\n",
    "    loan_individual_filled_df['verification_status'].isin(verified_values),\n",
    "    'Verified',\n",
    "    'Not Verified'\n",
    ")\n",
    "\n",
    "loan_individual_filled_df.drop(['verification_status'], axis=1,inplace=True) # to drop 'verification_status' after conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b668a46-d62f-494f-9d33-7784d585c1f7",
   "metadata": {},
   "source": [
    "#### 'region' of the US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05909184-3bb3-49ce-9bb0-4100163a93f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lists of states by region\n",
    "regions = {\n",
    "    'Northeast': ['CT','ME','MA','NH','RI','VT','NJ','NY','PA'],\n",
    "    'Midwest':   ['IN','IL','MI','OH','WI','IA','KS','MN','MO','NE','ND','SD'],\n",
    "    'South':     ['DE','FL','GA','MD','NC','SC','VA','DC','WV','AL','KY','MS','TN','AR','LA','OK','TX'],\n",
    "    'West':      ['AZ','CO','ID','MT','NV','NM','UT','WY','AK','CA','HI','OR','WA']\n",
    "}\n",
    "\n",
    "# Mapping US states to their regions\n",
    "state_to_region = {\n",
    "    state: region\n",
    "    for region, states in regions.items()\n",
    "    for state in states\n",
    "}\n",
    "\n",
    "# Apply the mapping\n",
    "loan_individual_filled_df['region'] = loan_individual_filled_df['addr_state'].map(state_to_region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81c87dc-cfb0-43a9-be80-5505bf1d30c4",
   "metadata": {},
   "source": [
    "#### 'avg_FICO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f2d844-e452-4105-82c5-10f2d9191db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_individual_filled_df['avg_FICO'] = (loan_individual_filled_df['fico_range_low'] + loan_individual_filled_df['fico_range_high'])/2\n",
    "\n",
    "# to drop 'fico_range_low' and 'fico_range_high' after conversion\n",
    "loan_individual_filled_df.drop(['fico_range_low', 'fico_range_high'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16dc0bd-6f65-4836-852c-6d151977de39",
   "metadata": {},
   "source": [
    "#### 'avg_last_FICO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf388bb-9beb-4153-9fa3-a4ca4c36f716",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_individual_filled_df['avg_last_FICO'] = (loan_individual_filled_df['last_fico_range_low'] + loan_individual_filled_df['last_fico_range_high'])/2\n",
    "\n",
    "# to drop 'last_fico_range_low' and 'last_fico_range_high' after conversion\n",
    "loan_individual_filled_df.drop(['last_fico_range_low', 'last_fico_range_high'], axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f267dfb-b8be-4c19-bec7-d15c77e002f7",
   "metadata": {},
   "source": [
    "#### 'median_household_income_2019' and 'rank_of_state_by_median_household_income_2019'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a06c48-ee28-4e23-a47d-cabca2182ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_individual_filled_df[\"addr_state\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f6e586-8bc7-4445-8919-39e2e618dcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_individual_filled_df[\"addr_state\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba3369-038c-49b8-9935-d158524ed95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load the ACS S1901 table (2019 1‑year estimates)\n",
    "acs = pd.read_csv('./Credit Risk_Lending Club/ACSST1Y2019.S1901-2025-04-20T165846.csv',dtype=str)\n",
    "\n",
    "# 2. Isolate the row containing the median household income estimate\n",
    "median_row = acs.loc[acs['Label (Grouping)'] == 'Median income (dollars)']\n",
    "\n",
    "# 3. Transpose so each state becomes a row\n",
    "med = (\n",
    "    median_row\n",
    "    .set_index('Label (Grouping)')\n",
    "    .T\n",
    "    .reset_index()\n",
    "    .rename(columns={'index': 'state_field', 0: 'Median_Household_Income'})\n",
    ")\n",
    "\n",
    "# 4. split state_field into state / category / measure\n",
    "med[['state','category','measure']] = (\n",
    "    med['state_field']\n",
    "       .str.split('!!', expand=True)\n",
    ")\n",
    "\n",
    "# 5. Clean and convert income to numeric\n",
    "med['Median_Household_Income'] = med['Median income (dollars)'].str.replace(',', '').astype(int)\n",
    "\n",
    "# 6. Keep only the rows where 'category' == 'Households'\n",
    "households_income = med[(med['category'].str.lower() == 'households')].copy()\n",
    "\n",
    "# 7. Map full state names to USPS abbreviations\n",
    "us_state_abbrev = {\n",
    "    'Alabama': 'AL','Alaska': 'AK','Arizona': 'AZ','Arkansas': 'AR',\n",
    "    'California': 'CA','Colorado': 'CO','Connecticut': 'CT','Delaware': 'DE',\n",
    "    'District of Columbia': 'DC','Florida': 'FL','Georgia': 'GA','Hawaii': 'HI',\n",
    "    'Idaho': 'ID','Illinois': 'IL','Indiana': 'IN','Iowa': 'IA',\n",
    "    'Kansas': 'KS','Kentucky': 'KY','Louisiana': 'LA','Maine': 'ME',\n",
    "    'Maryland': 'MD','Massachusetts': 'MA','Michigan': 'MI','Minnesota': 'MN',\n",
    "    'Mississippi': 'MS','Missouri': 'MO','Montana': 'MT','Nebraska': 'NE',\n",
    "    'Nevada': 'NV','New Hampshire': 'NH','New Jersey': 'NJ','New Mexico': 'NM',\n",
    "    'New York': 'NY','North Carolina': 'NC','North Dakota': 'ND','Ohio': 'OH',\n",
    "    'Oklahoma': 'OK','Oregon': 'OR','Pennsylvania': 'PA','Rhode Island': 'RI',\n",
    "    'South Carolina': 'SC','South Dakota': 'SD','Tennessee': 'TN','Texas': 'TX',\n",
    "    'Utah': 'UT','Vermont': 'VT','Virginia': 'VA','Washington': 'WA',\n",
    "    'West Virginia': 'WV','Wisconsin': 'WI','Wyoming': 'WY','Puerto Rico': 'PR'\n",
    "}\n",
    "households_income['state_abbrev'] = households_income['state'].map(us_state_abbrev)\n",
    "\n",
    "# 8. Build a mapping dictionary\n",
    "income_map = dict(zip(households_income['state_abbrev'], households_income['Median_Household_Income']))\n",
    "\n",
    "# 9. Map the median income into your loans DataFrame\n",
    "loan_individual_filled_df['median_household_income_2019'] = loan_individual_filled_df['addr_state'].map(income_map)\n",
    "\n",
    "# 10. Rank in descending order (highest income = rank 1)\n",
    "loan_individual_filled_df['rank_of_state_by_median_household_income_2019'\n",
    "] = (\n",
    "    loan_individual_filled_df['median_household_income_2019'\n",
    "].rank(ascending=False, method='dense').astype(int)\n",
    ")\n",
    "\n",
    "# 11. Print any state that does not have a rank\n",
    "missing = loan_individual_filled_df.loc[\n",
    "loan_individual_filled_df[\"rank_of_state_by_median_household_income_2019\"].isna(), \"addr_state\"].unique()\n",
    "if len(missing):\n",
    "    print(\"Warning—these addr_state codes weren’t found in the median‑income map:\", missing)\n",
    "    \n",
    "# View the first few rows to verify\n",
    "loan_individual_filled_df[['addr_state', 'median_household_income_2019','rank_of_state_by_median_household_income_2019']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be70b53-9fd3-4a55-88fe-2b857bea1f43",
   "metadata": {},
   "source": [
    "#### 'annual_income_more_than_median_household_income_2019'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ed5ddf-bdf4-403a-95df-a419af08fd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_individual_filled_df['annual_income_more_than_median_household_income_2019'] = (\n",
    "    loan_individual_filled_df['annual_inc'] > loan_individual_filled_df['median_household_income_2019']\n",
    ").map({True: 'yes', False: 'no'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b445c3-a6cc-4eff-a1d4-565f8f4d7c2c",
   "metadata": {},
   "source": [
    "#### '%_more_or_less'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3855c-a621-42a0-ab73-87b845e78bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_individual_filled_df['%_more_or_less_than_median'] = (\n",
    "    (loan_individual_filled_df['annual_inc'] - loan_individual_filled_df['median_household_income_2019']) \n",
    "    / loan_individual_filled_df['median_household_income_2019'] \n",
    "    * 100\n",
    ").round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c1217a-bfe5-4cbe-894a-71d329f7dff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify new columns\n",
    "loan_individual_filled_df[['annual_inc', 'median_household_income_2019',\n",
    "    'annual_income_more_than_median_household_income_2019', '%_more_or_less_than_median']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2d540f-6845-4f11-adb9-67531c72ca9a",
   "metadata": {},
   "source": [
    "#### 'number_of_months_bet_issued_and_earliest_cr_line'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffe7008-30c6-4099-bf0b-b127e74d70bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_individual_filled_df['number_of_months_bet_issued_and_earliest_cr_line'] = (\n",
    "    (loan_individual_filled_df['issue_d'].dt.year - loan_individual_filled_df['earliest_cr_line'].dt.year) * 12\n",
    "  + (loan_individual_filled_df['issue_d'].dt.month - loan_individual_filled_df['earliest_cr_line'].dt.month)\n",
    ")\n",
    "\n",
    "# 4) (Optional) Drop the temp datetime columns\n",
    "# df.drop(columns=['issue_date','earliest_cr_line_date'], inplace=True)\n",
    "\n",
    "loan_individual_filled_df[['issue_d','earliest_cr_line','number_of_months_bet_issued_and_earliest_cr_line']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1bc66b-5c3c-4d04-a02a-d0dc8fc71d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_loan_individual_filled_df = loan_individual_filled_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d2e673-c5fb-48b2-ae1a-24abf9babb20",
   "metadata": {},
   "source": [
    "### Preliminary Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3700d5-c36d-4e51-8be9-2cee88c3c80a",
   "metadata": {},
   "source": [
    "#### Breakdown of 'updated_loan_status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40789f2-5263-47d4-9e63-5434cf3fec8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate normalized distribution (percentages) for the loan status column.\n",
    "updated_loan_status_breakdown = cleaned_loan_individual_filled_df['updated_loan_status'].value_counts(normalize=True).reset_index()\n",
    "updated_loan_status_breakdown.columns = ['updated_loan_status', 'percentage']\n",
    "updated_loan_status_breakdown['percentage'] *= 100  # Convert to actual percentage\n",
    "\n",
    "# Create an interactive bar chart\n",
    "updated_loan_status_fig = px.bar(\n",
    "    updated_loan_status_breakdown,\n",
    "    x='updated_loan_status',\n",
    "    y='percentage',\n",
    "    labels={'loan_status_breakdown': 'Updated_Loan Status', 'percentage': 'Percentage'},\n",
    "    title='Breakdown of updated_loan_status Column',\n",
    "    text='percentage'\n",
    ")\n",
    "\n",
    "# Customize text display and rotate x-axis labels for better readability.\n",
    "updated_loan_status_fig.update_traces(texttemplate='%{text:.2f}%', textposition='outside', cliponaxis=False)\n",
    "updated_loan_status_fig.update_layout(xaxis_tickangle=0)\n",
    "\n",
    "# Display the interactive plot\n",
    "updated_loan_status_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f23c7e7-dc05-4d1a-b32b-4037919b4a05",
   "metadata": {},
   "source": [
    "#### 'updated_loan_status' by 'loan_amnt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d91d51-8540-44c8-992e-020969c0709c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a box plot of loan amount by updated loan status\n",
    "fig = px.box(\n",
    "    cleaned_loan_individual_filled_df,\n",
    "    x=\"updated_loan_status\",\n",
    "    y=\"loan_amnt\",\n",
    "    title=\"Distribution of Loan Amounts by Updated Loan Status\",\n",
    "    labels={\n",
    "        \"updated_loan_status\": \"Updated Loan Status\",\n",
    "        \"loan_amnt\": \"Loan Amount (USD)\"\n",
    "    }\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc8a6c-6332-49b7-b284-6dec8738613a",
   "metadata": {},
   "source": [
    "#### Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658ced66-5b8d-4369-a754-d1651e9d68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure size\n",
    "plt.figure(figsize=(18, 12))  # Increased size for better readability\n",
    "\n",
    "# Generate the heatmap with improved settings\n",
    "sns.heatmap(cleaned_loan_individual_filled_df.corr(numeric_only=True),\n",
    "            annot=True,               # Display values on the heatmap\n",
    "            cmap='viridis',           # Color map\n",
    "            annot_kws={\"size\": 10},   # Adjust annotation size\n",
    "            fmt='.2f',                # Format annotation to 2 decimal places\n",
    "            linewidths=0.5,           # Add some spacing between the cells\n",
    "            cbar_kws={'shrink': 0.8}, # Reduce the colorbar size\n",
    "            xticklabels=cleaned_loan_individual_filled_df.corr(numeric_only=True).columns,  # Ensure correct column labels\n",
    "            yticklabels=cleaned_loan_individual_filled_df.corr(numeric_only=True).columns   # Ensure correct row labels\n",
    "            )\n",
    "\n",
    "# Rotate x and y axis labels for better visibility\n",
    "plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "plt.yticks(rotation=0, ha='right', fontsize=12)\n",
    "\n",
    "# Title for the heatmap\n",
    "plt.title('Correlation of Features', fontsize=16)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()  # Adjust layout to ensure no clipping\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36fe49b0-8a01-43ab-b63b-1ad94828f16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Compute the absolute correlation matrix\n",
    "corr_matrix = cleaned_loan_individual_filled_df.corr(numeric_only=True).abs()\n",
    "\n",
    "# 2. Build a mask to exclude self‑correlations (the diagonal)\n",
    "mask = np.eye(corr_matrix.shape[0], dtype=bool)\n",
    "\n",
    "# 3. Unstack the off‑diagonal entries into a long table\n",
    "corr_pairs = (\n",
    "    corr_matrix\n",
    "      .where(~mask)      # drop diagonal\n",
    "      .stack()           # keep only non‑NaN entries\n",
    "      .reset_index()     # to DataFrame\n",
    ")\n",
    "corr_pairs.columns = ['Feature 1', 'Feature 2', 'Correlation']\n",
    "\n",
    "# 4. Filter for high correlations (e.g. > 0.6)\n",
    "threshold = 0.6\n",
    "high_corr = corr_pairs[corr_pairs['Correlation'] > threshold].copy()\n",
    "\n",
    "# 5. Add original column index for each feature\n",
    "high_corr['idx1'] = high_corr['Feature 1'].map(lambda f: loan_individual_filled_df.columns.get_loc(f))\n",
    "high_corr['idx2'] = high_corr['Feature 2'].map(lambda f: loan_individual_filled_df.columns.get_loc(f))\n",
    "\n",
    "# 6. Remove duplicate pairs (so (A,B) & (B,A) appear only once)\n",
    "high_corr['Ordered_Pair'] = high_corr.apply(\n",
    "    lambda r: tuple(sorted((r['Feature 1'], r['Feature 2']))),\n",
    "    axis=1\n",
    ")\n",
    "high_corr = (\n",
    "    high_corr\n",
    "      .drop_duplicates('Ordered_Pair')\n",
    "      .drop(columns='Ordered_Pair')\n",
    "      .sort_values('Correlation', ascending=False)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "# 7. Reorder columns for clarity\n",
    "high_corr = high_corr[['idx1','Feature 1','idx2','Feature 2','Correlation']]\n",
    "\n",
    "# 8. Print the result\n",
    "print(\"Highly correlated feature pairs (|corr| > 0.6) with original indices:\")\n",
    "print(high_corr.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd439adc-5891-4b06-af17-0535f84fee68",
   "metadata": {},
   "source": [
    "#### Distribution of 'loan_amt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c565514-7cf7-4c4f-9b9e-77a4104b2f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot interactive histogram of loan amounts\n",
    "loan_amt_fig = px.histogram(\n",
    "    cleaned_loan_individual_filled_df,\n",
    "    x=\"loan_amnt\",\n",
    "    nbins=50,\n",
    "    title=\"Distribution of loan_amt\",\n",
    "    labels={\"loan_amnt\": \"Loan Amount\", \"count\": \"Number of Loans\"}\n",
    ")\n",
    "\n",
    "loan_amt_fig.update_layout(\n",
    "    xaxis_title=\"Loan Amount\",\n",
    "    yaxis_title=\"Number of Loans\",\n",
    "    margin=dict(t=80, b=50)\n",
    ")\n",
    "\n",
    "loan_amt_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1b6b419-f877-456d-92ea-66d133bbf984",
   "metadata": {},
   "source": [
    "#### Distribution of 'loan_amt' by 'updated_loan_status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25200832-d153-4c20-93d8-3a398c02995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_df = cleaned_loan_individual_filled_df[cleaned_loan_individual_filled_df[\"updated_loan_status\"] == \"Good Loan\"]\n",
    "bad_df  = cleaned_loan_individual_filled_df[cleaned_loan_individual_filled_df[\"updated_loan_status\"] == \"Bad Loan\"]\n",
    "\n",
    "# Create subplots: 1 row, 2 columns\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"Good Loans\", \"Bad Loans\"),\n",
    "    shared_yaxes=True,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# Add histogram for Good Loans\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=good_df[\"loan_amnt\"],\n",
    "        histnorm=\"percent\",\n",
    "        nbinsx=50,\n",
    "        marker_color=\"green\",\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add histogram for Bad Loans\n",
    "fig.add_trace(\n",
    "    go.Histogram(\n",
    "        x=bad_df[\"loan_amnt\"],\n",
    "        histnorm=\"percent\",\n",
    "        nbinsx=50,\n",
    "        marker_color=\"red\",\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Distribution of Loan Amount by Updated Loan Status\",\n",
    "    barmode=\"overlay\",\n",
    "    margin=dict(t=80, b=50, l=50, r=50)\n",
    ")\n",
    "\n",
    "# Update axes labels\n",
    "fig.update_xaxes(title_text=\"Loan Amount (USD)\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Loan Amount (USD)\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Percentage of Loans (%)\", row=1, col=1)\n",
    "\n",
    "# Display the interactive figure\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cea629-863c-4f3e-8998-d6c5c43c8721",
   "metadata": {},
   "source": [
    "#### Breakdown of 'updated_loan_status' by 'int_rate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35852562-8cb0-4c37-a9eb-18a845230e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute count and percentage by interest rate and status\n",
    "grouped = (\n",
    "    cleaned_loan_individual_filled_df.groupby([\"int_rate\", \"updated_loan_status\"])\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "grouped[\"total\"] = grouped.groupby(\"int_rate\")[\"count\"].transform(\"sum\")\n",
    "grouped[\"percentage\"] = grouped[\"count\"] / grouped[\"total\"] * 100\n",
    "\n",
    "# Plot scatter of percentage by interest rate\n",
    "fig = px.scatter(\n",
    "    grouped,\n",
    "    x=\"int_rate\",\n",
    "    y=\"percentage\",\n",
    "    color=\"updated_loan_status\",\n",
    "    title=\"Percentage of Good vs. Bad Loans by Interest Rate\",\n",
    "    labels={\n",
    "        \"int_rate\": \"Interest Rate (%)\",\n",
    "        \"percentage\": \"Percentage of Loans (%)\",\n",
    "        \"updated_loan_status\": \"Loan Status\"\n",
    "    },\n",
    "    color_discrete_map={\"Good Loan\": \"green\", \"Bad Loan\": \"red\"},\n",
    "    symbol=\"updated_loan_status\",\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis=dict(tickformat=\".1f\"),\n",
    "    yaxis=dict(range=[0, 100]),\n",
    "    legend_title_text=\"Loan Status\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8ee26e-8344-43c5-9bf6-6e11afecced0",
   "metadata": {},
   "source": [
    "#### 'int_rate' by 'grade' and 'sub_grade'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c156105-76a8-48a0-a126-14ac31da741c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute average interest rate by grade and sub_grade\n",
    "avg_by_grade = cleaned_loan_individual_filled_df.groupby('grade')['int_rate'].mean()\n",
    "avg_by_sub_grade = cleaned_loan_individual_filled_df.groupby('sub_grade')['int_rate'].mean()\n",
    "\n",
    "# Create subplots: 1 row, 2 columns\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"Average Interest Rate by Grade\", \"Average Interest Rate by Sub_Grade\"),\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# Add line traces\n",
    "fig.add_trace(go.Scatter(x=avg_by_grade.index,     y=avg_by_grade.values,     mode='lines+markers', name='Grade'),     row=1, col=1)\n",
    "fig.add_trace(go.Scatter(x=avg_by_sub_grade.index, y=avg_by_sub_grade.values, mode='lines+markers', name='Sub_Grade'), row=1, col=2)\n",
    "\n",
    "# Layout\n",
    "fig.update_layout(title=\"Average Interest Rate by Grade & Sub-Grade\", showlegend=False)\n",
    "fig.update_xaxes(title_text=\"Grade\",      row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Sub-Grade\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Avg Interest Rate (%)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Avg Interest Rate (%)\", row=1, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2e4dba-9414-4fe5-b575-72605a38cf8e",
   "metadata": {},
   "source": [
    "#### 'updated_loan_status' by 'grade' and 'sub_grade'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c91725d-e3b5-4ac6-80fe-d57114f65aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute percentage of Bad Loans by grade\n",
    "grp_grade = (\n",
    "    cleaned_loan_individual_filled_df.groupby(['grade', 'updated_loan_status'], observed=True)\n",
    "      .size()\n",
    "      .reset_index(name='count')\n",
    ")\n",
    "grp_grade['total'] = grp_grade.groupby('grade', observed=True)['count'].transform('sum')\n",
    "grp_grade['percent'] = grp_grade['count'] / grp_grade['total'] * 100\n",
    "bad_pct_grade = grp_grade[grp_grade['updated_loan_status'] == 'Bad Loan']\n",
    "\n",
    "# Compute percentage of Bad Loans by sub_grade\n",
    "grp_sub = (\n",
    "    cleaned_loan_individual_filled_df.groupby(['sub_grade', 'updated_loan_status'], observed=True)\n",
    "      .size()\n",
    "      .reset_index(name='count')\n",
    ")\n",
    "grp_sub['total'] = grp_sub.groupby('sub_grade', observed=True)['count'].transform('sum')\n",
    "grp_sub['percent'] = grp_sub['count'] / grp_sub['total'] * 100\n",
    "bad_pct_sub = grp_sub[grp_sub['updated_loan_status'] == 'Bad Loan']\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"Bad Loan % by Grade\", \"Bad Loan % by Sub_Grade\"),\n",
    "    horizontal_spacing=0.12\n",
    ")\n",
    "\n",
    "# Add line traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=bad_pct_grade['grade'],\n",
    "        y=bad_pct_grade['percent'],\n",
    "        mode='lines+markers',\n",
    "        line=dict(width=2),\n",
    "        marker=dict(size=6),\n",
    "        name='Bad % by Grade'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=bad_pct_sub['sub_grade'],\n",
    "        y=bad_pct_sub['percent'],\n",
    "        mode='lines+markers',\n",
    "        line=dict(width=2),\n",
    "        marker=dict(size=4),\n",
    "        name='Bad % by Sub_Grade'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title_text=\"Percentage of Bad Loans by Grade & Sub_Grade\",\n",
    "    showlegend=False,\n",
    "    margin=dict(t=80, b=50, l=50, r=50)\n",
    ")\n",
    "fig.update_xaxes(title_text=\"Grade\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Sub_Grade\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Percentage of Bad Loans (%)\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"Percentage of Bad Loans (%)\", row=1, col=2)\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57609501-19e1-406f-9f78-c98d6cfc6c38",
   "metadata": {},
   "source": [
    "#### Breakdown of 'loan_status' by 'purpose'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb0ea70-a5a6-47ea-99d5-e436c5cab136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute counts & percentages by purpose\n",
    "grp = (\n",
    "    cleaned_loan_individual_filled_df.groupby([\"purpose\",\"updated_loan_status\"], observed=True)\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "grp[\"total\"]   = grp.groupby(\"purpose\", observed=True)[\"count\"].transform(\"sum\")\n",
    "grp[\"percent\"] = grp[\"count\"] / grp[\"total\"] * 100\n",
    "\n",
    "# Determine x-axis order by descending Bad‐Loan %\n",
    "bad_pct = grp[grp[\"updated_loan_status\"]==\"Bad Loan\"][[\"purpose\",\"percent\"]]\n",
    "purpose_order = bad_pct.sort_values(\"percent\", ascending=False)[\"purpose\"].tolist()\n",
    "\n",
    "# Plot without barnorm (y=\"percent\" directly)\n",
    "fig = px.bar(\n",
    "    grp,\n",
    "    x=\"purpose\",\n",
    "    y=\"percent\",\n",
    "    color=\"updated_loan_status\",\n",
    "    barmode=\"group\",\n",
    "    category_orders={\"purpose\": purpose_order},\n",
    "    color_discrete_map={\"Good Loan\":\"green\",\"Bad Loan\":\"red\"},\n",
    "    title=\"Percentage of Good vs. Bad Loans by Purpose\",\n",
    "    labels={\n",
    "      \"purpose\":\"Loan Purpose\",\n",
    "      \"updated_loan_status\":\"Loan Status\",\n",
    "      \"percent\":\"Percentage of Loans (%)\"\n",
    "    },\n",
    "    text=\"percent\"\n",
    ")\n",
    "\n",
    "# Format text and layout\n",
    "fig.update_traces(texttemplate='%{text:.2f}%', textposition='outside')\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-45,\n",
    "    yaxis=dict(title=\"Percentage of Loans (%)\", range=[0,100]),\n",
    "    legend_title_text=\"Loan Status\",\n",
    "    margin=dict(t=80, b=150)\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ae8e01-5088-4c25-9b2f-2f0d0b81d95a",
   "metadata": {},
   "source": [
    "#### Distribution of 'annual_inc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e274010b-eed0-4fe5-9f9c-91629f28dd00",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_loan_individual_filled_df['annual_inc'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c807c9d-6969-41d9-96be-1ac179a9db5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for annual income up to 75 percentile (i.e. less than 100,000)\n",
    "df_filtered = cleaned_loan_individual_filled_df[cleaned_loan_individual_filled_df['annual_inc'] < 100000]\n",
    "\n",
    "# Plot the distribution\n",
    "fig = px.histogram(\n",
    "    df_filtered,\n",
    "    x='annual_inc',\n",
    "    nbins=50,\n",
    "    title='Distribution of Annual Income (< $100,000)',\n",
    "    labels={'annual_inc': 'Annual Income (USD)', 'count': 'Number of Loans'}\n",
    ")\n",
    "fig.update_layout(xaxis_title='Annual Income (USD)', yaxis_title='Number of Loans')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce4ada3-370c-42ef-a59b-5fe2c8a4fc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for annual income less than 1,000,000\n",
    "df_filtered = cleaned_loan_individual_filled_df[cleaned_loan_individual_filled_df['annual_inc'] < 1000000]\n",
    "\n",
    "# Plot the distribution\n",
    "fig = px.histogram(\n",
    "    df_filtered,\n",
    "    x='annual_inc',\n",
    "    nbins=500,\n",
    "    title='Distribution of Annual Income (< $1,000,000)',\n",
    "    labels={'annual_inc': 'Annual Income (USD)', 'count': 'Number of Loans'}\n",
    ")\n",
    "fig.update_layout(xaxis_title='Annual Income (USD)', yaxis_title='Number of Loans')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99b456b-3788-4101-a2df-71e708898807",
   "metadata": {},
   "source": [
    "#### Breakdown of 'loan_status' by 'annual_inc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00600811-17d1-49c2-8552-454db61b7830",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped = (\n",
    "    cleaned_loan_individual_filled_df.groupby([\"annual_inc\", \"updated_loan_status\"], observed=True)\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "grouped[\"total\"] = grouped.groupby(\"annual_inc\", observed=True)[\"count\"].transform(\"sum\")\n",
    "grouped[\"percentage\"] = grouped[\"count\"] / grouped[\"total\"] * 100\n",
    "\n",
    "# Plot scatter of percentage breakdown by annual income\n",
    "fig = px.scatter(\n",
    "    grouped,\n",
    "    x=\"annual_inc\",\n",
    "    y=\"percentage\",\n",
    "    color=\"updated_loan_status\",\n",
    "    title=\"Percentage of Good vs. Bad Loans by Annual Income\",\n",
    "    labels={\n",
    "        \"annual_inc\": \"Annual Income (USD)\",\n",
    "        \"percentage\": \"Percentage of Loans (%)\",\n",
    "        \"updated_loan_status\": \"Loan Status\"\n",
    "    },\n",
    "    color_discrete_map={\"Good Loan\": \"green\", \"Bad Loan\": \"red\"},\n",
    "    symbol=\"updated_loan_status\",\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Annual Income (USD)\",\n",
    "    yaxis=dict(title=\"Percentage of Loans (%)\", range=[0, 100]),\n",
    "    legend_title_text=\"Loan Status\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2750df94-bec6-498f-a242-d1cbc3061d96",
   "metadata": {},
   "source": [
    "#### Breakdown of 'loan_status' by 'dti'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d78192-917c-4248-9d01-e26d64247b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by dti and status to compute percentage breakdown\n",
    "grouped = (\n",
    "    cleaned_loan_individual_filled_df.groupby([\"dti\", \"updated_loan_status\"], observed=True)\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "grouped[\"total\"] = grouped.groupby(\"dti\", observed=True)[\"count\"].transform(\"sum\")\n",
    "grouped[\"percentage\"] = grouped[\"count\"] / grouped[\"total\"] * 100\n",
    "\n",
    "# Plot scatter of percentage breakdown by DTI\n",
    "fig = px.scatter(\n",
    "    grouped,\n",
    "    x=\"dti\",\n",
    "    y=\"percentage\",\n",
    "    color=\"updated_loan_status\",\n",
    "    title=\"Percentage of Good vs. Bad Loans by Debt-to-Income Ratio (DTI)\",\n",
    "    labels={\n",
    "        \"dti\": \"Debt-to-Income Ratio (DTI)\",\n",
    "        \"percentage\": \"Percentage of Loans (%)\",\n",
    "        \"updated_loan_status\": \"Loan Status\"\n",
    "    },\n",
    "    color_discrete_map={\"Good Loan\": \"green\", \"Bad Loan\": \"red\"},\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"DTI\",\n",
    "    yaxis_title=\"Percentage of Loans (%)\",\n",
    "    legend_title_text=\"Loan Status\",\n",
    "    yaxis=dict(range=[0, 100])\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de7658-7c2b-4de9-b731-fa78d39a8c1d",
   "metadata": {},
   "source": [
    "#### Breakdown of 'updated_loan_status' by 'avg_FICO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebbc1d-453c-44ee-b213-30aae1b92ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by avg_FICO and status to compute percentage breakdown\n",
    "grouped = (\n",
    "    cleaned_loan_individual_filled_df.groupby([\"avg_FICO\", \"updated_loan_status\"], observed=True)\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "grouped[\"total\"] = grouped.groupby(\"avg_FICO\", observed=True)[\"count\"].transform(\"sum\")\n",
    "grouped[\"percentage\"] = grouped[\"count\"] / grouped[\"total\"] * 100\n",
    "\n",
    "# Plot scatter of percentage breakdown by DTI\n",
    "fig = px.scatter(\n",
    "    grouped,\n",
    "    x=\"avg_FICO\",\n",
    "    y=\"percentage\",\n",
    "    color=\"updated_loan_status\",\n",
    "    symbol='updated_loan_status',\n",
    "    title=\"Percentage of Good vs. Bad Loans by Average FICO Score\",\n",
    "    labels={\n",
    "        \"avg_FICO\": \"Average FICO Score\",\n",
    "        \"percentage\": \"Percentage of Loans (%)\",\n",
    "        \"updated_loan_status\": \"Loan Status\"\n",
    "    },\n",
    "    color_discrete_map={\"Good Loan\": \"green\", \"Bad Loan\": \"red\"},\n",
    "    opacity=0.7\n",
    ")\n",
    "\n",
    "fig.update_layout(yaxis=dict(range=[0,100]),\n",
    "                  legend_title_text='Loan Status')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919208ef-ee31-43b4-a570-fc10f0214081",
   "metadata": {},
   "source": [
    "#### 'updated_loan_status' by 'issue_d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bf28ac-2fc3-4497-93d8-20f6ac40624c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute percentage of bad loans by issue date\n",
    "grouped = (\n",
    "    cleaned_loan_individual_filled_df.groupby('issue_d')['bad_loan']\n",
    "      .mean()\n",
    "      .mul(100)\n",
    "      .reset_index(name='pct_bad_loans')\n",
    ")\n",
    "\n",
    "# Plot line chart\n",
    "fig = px.line(\n",
    "    grouped,\n",
    "    x='issue_d',\n",
    "    y='pct_bad_loans',\n",
    "    markers=True,\n",
    "    title='Percentage of Bad Loans Over Time (by Issue Date)',\n",
    "    labels={\n",
    "        'issue_d': 'Issue Date',\n",
    "        'pct_bad_loans': 'Percentage of Bad Loans (%)'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Retain datetime formatting on the x-axis\n",
    "fig.update_xaxes(tickformat='%b-%Y')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b22e4-ce06-424d-bf3c-1f58c6135048",
   "metadata": {},
   "source": [
    "#### Breakdown of 'updated_loan_status' by 'delinq_2_years'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b410cc0e-b421-490b-b376-addd472c9174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute count and percentage breakdown by delinq_2yrs and status\n",
    "grouped = (\n",
    "    cleaned_loan_individual_filled_df.groupby(['delinq_2yrs', 'updated_loan_status'], observed=True)\n",
    "      .size()\n",
    "      .reset_index(name='count')\n",
    ")\n",
    "grouped['total'] = grouped.groupby('delinq_2yrs')['count'].transform('sum')\n",
    "grouped['percentage'] = grouped['count'] / grouped['total'] * 100\n",
    "\n",
    "# Sort by delinq_2yrs\n",
    "grouped = grouped.sort_values('delinq_2yrs')\n",
    "\n",
    "# Plot interactive line chart\n",
    "fig = px.line(\n",
    "    grouped,\n",
    "    x='delinq_2yrs',\n",
    "    y='percentage',\n",
    "    color='updated_loan_status',\n",
    "    markers=True,\n",
    "    title='Percentage of Good vs. Bad Loans by Number of Delinquencies (Last 2 Years)',\n",
    "    labels={\n",
    "        'delinq_2yrs': 'Delinquencies in Past 2 Years',\n",
    "        'percentage': 'Percentage of Loans (%)',\n",
    "        'updated_loan_status': 'Loan Status'\n",
    "    },\n",
    "    color_discrete_map={'Good Loan': 'green', 'Bad Loan': 'red'}\n",
    ")\n",
    "fig.update_layout(\n",
    "    xaxis=dict(dtick=1),\n",
    "    yaxis=dict(range=[0, 100]),\n",
    "    legend_title_text='Loan Status',\n",
    "    margin=dict(t=80, b=50)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab608d0b-8308-4d1a-b7fd-ce6cde3981c7",
   "metadata": {},
   "source": [
    "#### Breakdown of 'updated_loan_status' by 'addr_state'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4991c92-269d-4a02-9b56-ccbd85ad1491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute % breakdown by state\n",
    "grp = (\n",
    "    cleaned_loan_individual_filled_df.groupby([\"addr_state\", \"updated_loan_status\"], observed=True)\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    ")\n",
    "grp[\"total\"] = grp.groupby(\"addr_state\", observed=True)[\"count\"].transform(\"sum\")\n",
    "grp[\"percent\"] = grp[\"count\"] / grp[\"total\"] * 100\n",
    "\n",
    "# Pivot and sort states by descending Bad‐Loan %\n",
    "pivot = (\n",
    "    grp.pivot(index=\"addr_state\", columns=\"updated_loan_status\", values=\"percent\")\n",
    "       .fillna(0)\n",
    "       .reset_index()\n",
    ")\n",
    "pivot = pivot.sort_values(\"Bad Loan\", ascending=False)\n",
    "\n",
    "# Melt for plotting\n",
    "plot_df = pivot.melt(\n",
    "    id_vars=\"addr_state\",\n",
    "    value_vars=[\"Good Loan\", \"Bad Loan\"],\n",
    "    var_name=\"Loan Status\",\n",
    "    value_name=\"Percentage\"\n",
    ")\n",
    "\n",
    "# Draw grouped bar chart\n",
    "fig = px.bar(\n",
    "    plot_df,\n",
    "    x=\"addr_state\",\n",
    "    y=\"Percentage\",\n",
    "    color=\"Loan Status\",\n",
    "    barmode=\"group\",\n",
    "    category_orders={\"addr_state\": pivot[\"addr_state\"].tolist()},\n",
    "    color_discrete_map={\"Good Loan\": \"green\", \"Bad Loan\": \"red\"},\n",
    "    title=\"Percentage of Good vs. Bad Loans by State (sorted by Bad %)\",\n",
    "    labels={\n",
    "        \"addr_state\": \"State\",\n",
    "        \"Percentage\": \"Percentage of Loans (%)\",\n",
    "        \"Loan Status\": \"Loan Status\",\n",
    "    },\n",
    "    text=\"Percentage\"\n",
    ")\n",
    "fig.update_traces(texttemplate=\"%{text:.2f}%\", textposition=\"outside\")\n",
    "fig.update_layout(\n",
    "    xaxis_tickangle=-45,\n",
    "    yaxis=dict(title=\"Percentage of Loans (%)\", range=[0, 100]),\n",
    "    legend_title_text=\"Loan Status\",\n",
    "    margin=dict(t=100, b=150)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d55ee1-daa0-4074-9028-a72e4fedc712",
   "metadata": {},
   "source": [
    "#### Breakdown of 'updated_loan_status' by 'region' of the US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e14bc3-db1c-49ad-9007-bd7e5cb981ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute % by region\n",
    "grp = (cleaned_loan_individual_filled_df.groupby(['region','updated_loan_status'], observed=True)\n",
    "          .size()\n",
    "          .reset_index(name='count'))\n",
    "grp['total'] = grp.groupby('region', observed=True)['count'].transform('sum')\n",
    "grp['percentage'] = grp['count']/grp['total']*100\n",
    "\n",
    "# Plot\n",
    "fig = px.bar(\n",
    "  grp, x='region', y='percentage', color='updated_loan_status',\n",
    "  barmode='group',\n",
    "  category_orders={'region': ['Northeast','Midwest','South','West']},\n",
    "  color_discrete_map={'Good Loan':'green','Bad Loan':'red'},\n",
    "  title='% of Good vs. Bad Loans by Region',\n",
    "  labels={'percentage':'% of Loans','region':'Region','updated_loan_status':'Loan Status'},\n",
    "  text='percentage'\n",
    ")\n",
    "fig.update_traces(texttemplate='%{text:.2f}%', textposition='outside')\n",
    "fig.update_layout(yaxis=dict(range=[0,100]), xaxis_tickangle=-45)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2dbbf6-9351-4795-96c8-43259edb9674",
   "metadata": {},
   "source": [
    "#### Breakdown of 'updated_loan_status' by 'rank_of_state_by_median_household_income_2019'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c4e81c-37a9-42cf-92ed-1d1f5bb444ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by rank and status to compute counts & percentages\n",
    "grouped = (\n",
    "    cleaned_loan_individual_filled_df\n",
    "    .groupby(['rank_of_state_by_median_household_income_2019', 'updated_loan_status'], observed=True)\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "grouped['total'] = grouped.groupby('rank_of_state_by_median_household_income_2019', observed=True)['count'].transform('sum')\n",
    "grouped['percentage'] = grouped['count'] / grouped['total'] * 100\n",
    "\n",
    "# Sort by rank\n",
    "grouped = grouped.sort_values('rank_of_state_by_median_household_income_2019')\n",
    "\n",
    "# Plot breakdown as a line chart\n",
    "fig = px.line(\n",
    "    grouped,\n",
    "    x='rank_of_state_by_median_household_income_2019',\n",
    "    y='percentage',\n",
    "    color='updated_loan_status',\n",
    "    markers=True,\n",
    "    title='Percentage of Good vs. Bad Loans by State Income Rank (2019)',\n",
    "    labels={\n",
    "        'rank_of_state_by_median_household_income_2019': 'State Median Income Rank (1=Highest)',\n",
    "        'percentage': 'Percentage of Loans (%)',\n",
    "        'updated_loan_status': 'Loan Status'\n",
    "    },\n",
    "    color_discrete_map={'Good Loan': 'green', 'Bad Loan': 'red'}\n",
    ")\n",
    "fig.update_layout(yaxis=dict(range=[0, 100]))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fa1572-9ae7-4549-9582-fde7ad4a32f2",
   "metadata": {},
   "source": [
    "#### 'annual_income_more_than_median_household_income_2019' by 'loan status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182847f2-f65d-4a41-9935-77ab7cba9337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute percentage breakdown by the yes/no flag and loan status\n",
    "grp = (\n",
    "    cleaned_loan_individual_filled_df\n",
    "    .groupby(['annual_income_more_than_median_household_income_2019', 'updated_loan_status'], observed=True)\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "grp['total'] = grp.groupby('annual_income_more_than_median_household_income_2019', observed=True)['count'].transform('sum')\n",
    "grp['percentage'] = grp['count'] / grp['total'] * 100\n",
    "\n",
    "# 4. Plot grouped bar chart\n",
    "fig = px.bar(\n",
    "    grp,\n",
    "    x='annual_income_more_than_median_household_income_2019',\n",
    "    y='percentage',\n",
    "    color='updated_loan_status',\n",
    "    barmode='group',\n",
    "    category_orders={'annual_income_more_than_median_household_income_2019': ['yes', 'no']},\n",
    "    color_discrete_map={'Good Loan': 'green', 'Bad Loan': 'red'},\n",
    "    title='Loan Status Breakdown by Annual Income > State Median',\n",
    "    labels={\n",
    "        'annual_income_more_than_median_household_income_2019': 'Annual Income > Median Household Income?',\n",
    "        'percentage': 'Percentage of Loans (%)',\n",
    "        'updated_loan_status': 'Loan Status'\n",
    "    },\n",
    "    text='percentage'\n",
    ")\n",
    "fig.update_traces(texttemplate='%{text:.2f}%', textposition='outside')\n",
    "fig.update_layout(\n",
    "    yaxis=dict(title='Percentage of Loans (%)', range=[0, 100]),\n",
    "    xaxis_title='Annual Income > Median Household Income?',\n",
    "    legend_title='Loan Status',\n",
    "    margin=dict(t=80, b=50)\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f470c0-d9d6-48d9-9d9f-0cd89b9a5512",
   "metadata": {},
   "source": [
    "#### '%_more_or_less' against 'updated_loan_status'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "911fb162-9f8f-4d4d-8d66-83b051f12890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Group by % difference and status to compute counts & percentages\n",
    "grouped = (\n",
    "    cleaned_loan_individual_filled_df\n",
    "    .groupby(['%_more_or_less_than_median', 'updated_loan_status'], observed=True)\n",
    "    .size()\n",
    "    .reset_index(name='count')\n",
    ")\n",
    "grouped['total'] = grouped.groupby('%_more_or_less_than_median', observed=True)['count'].transform('sum')\n",
    "grouped['percentage'] = grouped['count'] / grouped['total'] * 100\n",
    "\n",
    "# 2. Plot scatter of percentage vs. % difference\n",
    "fig = px.scatter(\n",
    "    grouped,\n",
    "    x='%_more_or_less_than_median',\n",
    "    y='percentage',\n",
    "    color='updated_loan_status',\n",
    "    symbol='updated_loan_status',\n",
    "    opacity=0.7,\n",
    "    title='Percentage of Good vs. Bad Loans by % Difference from Median Income',\n",
    "    labels={\n",
    "        '%_more_or_less_than_median': '% Difference from Median Income',\n",
    "        'percentage': 'Percentage of Loans (%)',\n",
    "        'updated_loan_status': 'Loan Status'\n",
    "    },\n",
    "    color_discrete_map={'Good Loan': 'green', 'Bad Loan': 'red'}\n",
    ")\n",
    "fig.update_layout(\n",
    "    yaxis=dict(range=[0, 100]),\n",
    "    legend_title_text='Loan Status'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ae5c1c-a91e-4695-844d-59dd18e5e768",
   "metadata": {},
   "source": [
    "### Further Exploratory Data Analysis: Feature selection based on statistical significance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0244f-c23d-461d-b1d6-914539cdae80",
   "metadata": {},
   "source": [
    "#### Categorical - Continuous Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0385e866-e371-4743-b8e8-b127710d410c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e28c4499-a8db-4c78-ace8-9ac96fcd91e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_loan_individual_filled_df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc533ae0-01a1-47a7-879c-7567b9ed2184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting categorical features to run chi-square test\n",
    "cat_col = cleaned_loan_individual_filled_df.select_dtypes(include=['object'])\n",
    "print(cat_col.shape)\n",
    "cat_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a162b2-94b1-4b8f-9d20-99c1fd8905f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting continuous features to run anova/KS test\n",
    "cont_col = cleaned_loan_individual_filled_df.select_dtypes(include=['int64','float64'])\n",
    "cont_col.drop(['bad_loan'], axis=1,inplace=True)\n",
    "print(cont_col.shape)\n",
    "cont_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bd9045-1c6c-4811-84cb-34039b78fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining target variable, updated_loan_status into continuous features dataframe\n",
    "updated_loan_status = cleaned_loan_individual_filled_df[['updated_loan_status']]\n",
    "cont_col = cont_col.join(updated_loan_status)\n",
    "print(cont_col.shape)\n",
    "cont_col.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bc7ac4-a14b-4353-9d85-cdcffd726fe8",
   "metadata": {},
   "source": [
    "### Categorical features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c77975-1b2a-4d49-b547-b91b7d8c6d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding for categorical features with more than 2 values\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "cat_col_transformed_dict = {}\n",
    "\n",
    "for col in cat_col.columns:\n",
    "    cat_col_transformed = label_encoder.fit_transform(cat_col[col])\n",
    "    cat_col_transformed_dict[col] = cat_col_transformed\n",
    "    \n",
    "cat_col_transformed = pd.DataFrame(cat_col_transformed_dict)\n",
    "print(cat_col_transformed.shape)\n",
    "cat_col_transformed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c67a14-e946-4dc7-b8dc-509b90c637e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi-square test\n",
    "X = cat_col_transformed.drop(['updated_loan_status'],axis=1)\n",
    "y = cat_col_transformed['updated_loan_status']\n",
    "\n",
    "chi_scores = chi2(X,y)\n",
    "chi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345a0f00-7681-41eb-b9a6-1d9921fbc44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top correlated features + p_values visualization from chi-square test\n",
    "p_values = pd.Series(chi_scores[1],index = X.columns)\n",
    "p_values.sort_values(inplace = True)\n",
    "\n",
    "fig=px.bar(p_values, labels={'value': 'p_value'}, width=600, height=500)\n",
    "fig.add_hline(y=0.05, line_dash=\"dash\", line_color=\"red\")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_xaxes(tickangle = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8be3648-25f4-4c0e-b355-652c7be3c117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# top correlated features + p_values converted to dataframe for top features list extraction\n",
    "cat_pvals_df = pd.DataFrame(p_values, columns=['p_value'])\n",
    "cat_pvals_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c01f50-675c-4135-a672-7f3edc157632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of top correlated categorical features\n",
    "top_cat_features_list = cat_pvals_df[cat_pvals_df['p_value']<0.05].index.to_list()\n",
    "top_cat_features_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c5ef5-4a5f-41be-b1c4-8c3eee240060",
   "metadata": {},
   "source": [
    "### Continuous features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9271c6e2-4113-4dc1-92a7-a7c40b3ba11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting all continuous feature column\n",
    "cont_col_feature_cols = [col for col in cont_col.columns if col not in ['updated_loan_status']]\n",
    "print(len(cont_col_feature_cols))\n",
    "cont_col_feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affada35-3993-4b10-b8d4-b03d61565a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# running normality and statistical significance tests on continuous features\n",
    "results_dict = {col:cont_stat_sig_test(cont_col,col) for col in cont_col_feature_cols}\n",
    "\n",
    "cont_stat_results_df = pd.DataFrame.from_dict(results_dict, orient='index', columns=['p_value']).sort_values(by='p_value')\n",
    "\n",
    "fig=px.bar(cont_stat_results_df, labels={'value': 'p_value'}, width=600, height=500)\n",
    "fig.add_hline(y=0.05, line_dash=\"dash\", line_color=\"red\")\n",
    "fig.update_layout(showlegend=False)\n",
    "fig.update_xaxes(tickangle = 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416827d1-08f3-4df0-9b3a-08a43a20515b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous features with statistical significance correlation to response\n",
    "cont_stat_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c181cb65-7739-46bd-86ba-e5fe5ad2e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rearranging cont_col dataset's feature columns in the order of statistical significance\n",
    "cont_col = cont_col[['updated_loan_status']+cont_stat_results_df.index.tolist()]\n",
    "cont_feature_cols = cont_col.columns.tolist()\n",
    "cont_feature_cols.remove('updated_loan_status')\n",
    "print(cont_col.shape)\n",
    "cont_col.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ac736d-581e-4401-b01b-811b2d163a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_loan_individual_filled_df = cleaned_loan_individual_filled_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2b3e0e-a0c3-4d56-84bd-ae44e2b5ae0e",
   "metadata": {},
   "source": [
    "## Dummy Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641c737-4cca-4b2e-b20c-78c2db38d9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_loan_individual_filled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3239da-77c6-4e13-a5ba-20fa46b6dad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_loan_individual_filled_df.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e06452-8538-495f-8e7b-9c0d98eb2e9e",
   "metadata": {},
   "source": [
    "### grade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46723cca-8aab-430e-835e-f6f490d6b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sorting the ordinal features in correct order\n",
    "grade_to_numeric = {'A': 1, 'B': 1, 'C': 1, 'D': 1,\n",
    "                    'E': 2, 'F': 3, 'G': 4}\n",
    "\n",
    "# Create a new column with numeric grades\n",
    "modelling_loan_individual_filled_df['updated_grade'] = modelling_loan_individual_filled_df['grade'].map(grade_to_numeric)\n",
    "\n",
    "# Verify the mapping\n",
    "modelling_loan_individual_filled_df[['grade', 'updated_grade']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0271e6f0-f104-4193-97b4-d6259c278988",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_loan_individual_filled_df['updated_grade'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f0e8ac-c2eb-49d4-9fe5-b4323aeac8c7",
   "metadata": {},
   "source": [
    "### home_ownership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe87567-7702-4353-acbf-7513805643d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_loan_individual_filled_df['home_ownership'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d11f0a-e694-40f4-b7d9-6b203ab68e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy code the 'home_ownership' column\n",
    "\n",
    "# Create dummy variables for the 'home_ownership' column\n",
    "home_ownership_dummies = pd.get_dummies(modelling_loan_individual_filled_df.home_ownership, prefix='home_ownership')\n",
    "home_ownership_dummies.drop(home_ownership_dummies.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# If you want to add these new dummy columns back into your original DataFrame, use pd.concat:\n",
    "modelling_loan_individual_filled_df = pd.concat([modelling_loan_individual_filled_df, home_ownership_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f813a585-3d70-41d3-bcb9-0934fb5934f7",
   "metadata": {},
   "source": [
    "### purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc58ec1d-f815-43d5-a1da-9549ff56b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the three purposes to keep\n",
    "dummy_purpose = ['small_business', 'moving', 'renewable_energy']\n",
    "\n",
    "# Create a grouped‐purpose column\n",
    "modelling_loan_individual_filled_df['updated_purpose'] = modelling_loan_individual_filled_df['purpose'].where(modelling_loan_individual_filled_df['purpose'].isin(dummy_purpose), other='others')\n",
    "\n",
    "# One-hot encode that grouping\n",
    "purpose_dummies = pd.get_dummies(modelling_loan_individual_filled_df.updated_purpose, prefix='purpose')\n",
    "purpose_dummies.drop(purpose_dummies.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# Attach the dummies back to your DataFrame\n",
    "modelling_loan_individual_filled_df = pd.concat([modelling_loan_individual_filled_df, purpose_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4634cda-721c-48b1-bbc0-036b2fb3cfc7",
   "metadata": {},
   "source": [
    "### addr_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d46829a-fe9b-4b6d-ac08-b80faa224f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_loan_individual_filled_df['addr_state'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cff3c27-07c6-4459-a7b9-2ae32b6edb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummy code the 'addr_state' column\n",
    "\n",
    "# Create dummy variables for the 'home_ownership' column\n",
    "addr_state_dummies = pd.get_dummies(modelling_loan_individual_filled_df.addr_state, prefix='addr_state')\n",
    "addr_state_dummies.drop(addr_state_dummies.columns[0], axis=1, inplace=True)\n",
    "\n",
    "# If you want to add these new dummy columns back into your original DataFrame, use pd.concat:\n",
    "modelling_loan_individual_filled_df = pd.concat([modelling_loan_individual_filled_df, addr_state_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f22287-8fde-4305-8b76-4086c72906ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelling_loan_individual_filled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9561bd98-f510-4f72-b6bb-ac217dacf034",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309f26e6-a261-4df2-b376-3a894df3b5cd",
   "metadata": {},
   "source": [
    "### Feature Columns (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2f6442-9c71-4254-8509-0dda0f4937ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['loan_amnt','term','int_rate','installment', #'last_pymnt_amnt',\n",
    "                'updated_grade',# 'grade',# 'sub_grade',\n",
    "                'emp_length_int',\n",
    "                'home_ownership_MORTGAGE', 'home_ownership_NONE', 'home_ownership_OTHER', \n",
    "                'home_ownership_OWN',\t'home_ownership_RENT',# 'home_ownership',\n",
    "                'annual_inc',# 'cleaned_verification_status',\n",
    "\n",
    "                'purpose_others', 'purpose_renewable_energy', 'purpose_small_business',# 'purpose', 'updated_purpose'\n",
    "                \n",
    "                'dti','delinq_2yrs','avg_FICO',#'avg_last_FICO',\n",
    "                'inq_last_6mths',\n",
    "                \n",
    "                'open_acc','pub_rec',#'total_acc',\n",
    "                'revol_bal',#'tot_cur_bal','avg_cur_bal',\n",
    "                'revol_util',#'bc_open_to_buy','bc_util',\n",
    "                \n",
    "                # 'out_prncp','out_prncp_inv',\n",
    "                'total_pymnt',\n",
    "                # 'total_pymnt_inv',\n",
    "                # 'total_rec_prncp','total_rec_int',\n",
    "                'total_rec_late_fee',\n",
    "                # 'recoveries','collection_recovery_fee',\n",
    "                \n",
    "                'collections_12_mths_ex_med', \n",
    "                # 'acc_now_delinq',\n",
    "                'tot_coll_amt',\n",
    "                \n",
    "                'acc_open_past_24mths',\n",
    "                # 'chargeoff_within_12_mths',# 'delinq_amnt',\n",
    "                'mo_sin_old_il_acct','mo_sin_old_rev_tl_op',#'mo_sin_rcnt_rev_tl_op','mo_sin_rcnt_tl',\n",
    "                'mort_acc',#'mths_since_recent_bc',\n",
    "                \n",
    "                'num_accts_ever_120_pd','num_actv_bc_tl',#'num_actv_rev_tl','num_bc_sats','num_bc_tl','num_il_tl',\n",
    "                'num_op_rev_tl',#'num_rev_accts','num_rev_tl_bal_gt_0',\n",
    "                'num_sats',\n",
    "                # 'num_tl_120dpd_2m', # 'num_tl_30dpd',\n",
    "                'num_tl_90g_dpd_24m','num_tl_op_past_12m',\n",
    "                \n",
    "                'pct_tl_nvr_dlq','percent_bc_gt_75','pub_rec_bankruptcies','tax_liens','tot_hi_cred_lim',\n",
    "                'total_bal_ex_mort','total_bc_limit','total_il_high_credit_limit',\n",
    "                                \n",
    "                # 'hardship_flag',\n",
    "                # 'debt_settlement_flag',\n",
    "   \n",
    "                # 'median_household_income_2019',\n",
    "                'rank_of_state_by_median_household_income_2019',\n",
    "                # 'annual_income_more_than_median_household_income_2019',\n",
    "                '%_more_or_less_than_median'\n",
    "                # 'addr_state'\n",
    "                # 'addr_state_AL','addr_state_AR','addr_state_AZ','addr_state_CA','addr_state_CO','addr_state_CT',\n",
    "                # 'addr_state_DC','addr_state_DE','addr_state_FL','addr_state_GA','addr_state_HI','addr_state_IA', \n",
    "                # 'addr_state_ID', 'addr_state_IL', 'addr_state_IN', 'addr_state_KS', 'addr_state_KY', 'addr_state_LA', \n",
    "                # 'addr_state_MA', 'addr_state_MD', 'addr_state_ME', 'addr_state_MI', 'addr_state_MN', 'addr_state_MO', \n",
    "                # 'addr_state_MS', 'addr_state_MT', 'addr_state_NC', 'addr_state_ND', 'addr_state_NE', 'addr_state_NH', \n",
    "                # 'addr_state_NJ', 'addr_state_NM', 'addr_state_NV', 'addr_state_NY', 'addr_state_OH', 'addr_state_OK', \n",
    "                # 'addr_state_OR', 'addr_state_PA', 'addr_state_RI', 'addr_state_SC', 'addr_state_SD', 'addr_state_TN', \n",
    "                # 'addr_state_TX', 'addr_state_UT', 'addr_state_VA', 'addr_state_VT', 'addr_state_WA', 'addr_state_WI', \n",
    "                # 'addr_state_WV', 'addr_state_WY' \n",
    "               ]\n",
    "\n",
    "X = modelling_loan_individual_filled_df[feature_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9fa7d0-b7bb-44bc-bc85-b1eea742d7ad",
   "metadata": {},
   "source": [
    "### Target Variable (y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332fb0a1-b73e-4e16-bf07-11038ff54ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = modelling_loan_individual_filled_df['bad_loan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5e85f5-536b-4194-a759-ddcabbb2ac85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split, PredefinedSplit\n",
    "from sklearn.metrics import make_scorer, accuracy_score, recall_score, f1_score, precision_score, confusion_matrix, classification_report, precision_recall_curve\n",
    "\n",
    "# Split into train/test, preserving the imbalance via stratify\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,         # 20% for testing; adjust as needed\n",
    "    random_state=42,\n",
    "    stratify=y             # keeps the percentage of target variable consistent in both sets\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9118ccb-b183-47b5-b04f-fac67d8a2c41",
   "metadata": {},
   "source": [
    "### Modelling using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c69385e-1b0e-4867-9f97-6eb2536dfa90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline # load a tool that lets us string together several steps into one simple process\n",
    "from sklearn.preprocessing import StandardScaler  # load a tool that makes each column of numbers roughly the same size, so that they are balanced\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold  # load tools to try many different settings and to split our data in a way that keeps the same mix of good vs. bad loans each time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074fd972-6212-4a48-99c6-460e326dd7b8",
   "metadata": {},
   "source": [
    "#### Step 1: Training Base Logistic Regression Model without Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0a6f31-fef2-403c-9a97-3d81a1f8b71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize logistic regression with balanced class weights\n",
    "base_log_reg_model = LogisticRegression(\n",
    "    class_weight='balanced',  # weight the minority class more\n",
    "    solver='liblinear',       # good default for small-to-medium datasets\n",
    "    max_iter=1000,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1367138d-eee4-4654-8c8c-2e0657d7bf28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "base_log_reg_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = base_log_reg_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "print('\\nConfusion Matrix:')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7465f7-5a1e-47f0-bc2f-485a8afc0ba9",
   "metadata": {},
   "source": [
    "#### Step 2a: Hyperparameter Tuning for base_log_reg_model, optimising for Precision, whilst keeping runtime <30 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde64920-ee8e-4238-b847-19e6bc49fc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Split off 20% of your training data for validation, keeping the same 12.5% bad-loan rate:\n",
    "X_subtrain, X_val, y_subtrain, y_val = train_test_split(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    test_size=0.20,\n",
    "    stratify=y_train,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2) Tell sklearn which rows are “train” (–1) vs “validation” (0):\n",
    "test_fold = np.concatenate([\n",
    "    np.full(len(X_subtrain), -1, dtype=int),\n",
    "    np.zeros(len(X_val),       dtype=int)\n",
    "])\n",
    "ps = PredefinedSplit(test_fold)\n",
    "\n",
    "# 3) Our “scale → classify” recipe, with a lighter iteration cap:\n",
    "pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"clf\", LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        max_iter=300,             \n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "\n",
    "# 4) A much smaller knob-set to spin:\n",
    "param_distributions = [\n",
    "    {\n",
    "        \"clf__solver\": [\"liblinear\"],           \n",
    "        \"clf__penalty\": [\"l1\", \"l2\"], \n",
    "        \"clf__C\": [0.1, 1, 10]                   \n",
    "    }\n",
    "]\n",
    "\n",
    "# 5) Set up a much shorter randomized search:\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=pipe,\n",
    "    param_distributions=param_distributions, \n",
    "    n_iter=6,                                  \n",
    "    scoring=\"precision\",\n",
    "    cv=ps,\n",
    "    n_jobs=1,\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 6) Combine and run:\n",
    "X_combined = np.vstack([X_subtrain, X_val])\n",
    "y_combined = np.concatenate([y_subtrain, y_val])\n",
    "search.fit(X_combined, y_combined)\n",
    "\n",
    "# 7) Report validation‐fold precision:\n",
    "print(\"Best settings:\", search.best_params_)\n",
    "print(\"Best precision (validation):\", f\"{search.best_score_:.3f}\")\n",
    "\n",
    "# 8) Final test‐set check:\n",
    "best_log_reg_model = search.best_estimator_\n",
    "y_pred = best_log_reg_model.predict(X_test)\n",
    "\n",
    "print(f\"\\nTest accuracy : {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Test precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a15c0b6-7ce5-43ad-af9b-842ed1034e29",
   "metadata": {},
   "source": [
    "#### Step 3: Improve Precision Scores (i.e. Precision ≥ 0.7) using Threshold Calibration on 'base_log_reg_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f838e81b-97c7-49a9-a621-a1d9211bc29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get the predicted probability for the positive class on your test set\n",
    "y_probability = best_log_reg_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2) Compute the precision-recall curve\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, y_probability)\n",
    "\n",
    "# 3) Find all thresholds that give precision ≥ 0.7\n",
    "# Note: `precisions[i+1]` corresponds to `thresholds[i]`\n",
    "valid_idxs = np.where(precisions[1:] >= 0.7)[0]\n",
    "\n",
    "if len(valid_idxs) == 0:\n",
    "    raise ValueError(\"No threshold achieves precision ≥ 0.7\")\n",
    "\n",
    "# 4) Pick the threshold that maximizes recall among those\n",
    "best_idx = valid_idxs[np.argmax(recalls[1:][valid_idxs])]\n",
    "best_threshold = thresholds[best_idx]\n",
    "print(f\"Chosen probability threshold for precision ≥ 0.7: {best_threshold:.3f}\")\n",
    "\n",
    "# 5) Apply that threshold to get final predictions\n",
    "y_pred_threshold = (y_probability >= best_threshold).astype(int)\n",
    "\n",
    "# 6) Evaluate\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_threshold):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_threshold):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_threshold):.4f}\")\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_threshold))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb59f1d-16af-484f-8128-3f46d5bfa5f8",
   "metadata": {},
   "source": [
    "### Modelling using XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f848e4e-80e4-438a-a0bd-7b83c7466833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dbf26d-0269-412c-a4a1-3431162e13d8",
   "metadata": {},
   "source": [
    "#### Step 1: Training Base XGBoost Model without Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b2b869-39ce-495c-9ff1-c5b669ae28c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute scale_pos_weight to balance the classes\n",
    "# Counts how many examples in your training labels are “negative” (class 0) and how many are “positive” (class 1).\n",
    "neg, pos = np.bincount(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e0cb5b-7c4b-4d8b-b7c8-4bb68acbf517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the classifier\n",
    "base_xgb_model = XGBClassifier( \n",
    "    objective='binary:logistic', # Learning Task Parameter: for binary classification\n",
    "    scale_pos_weight=neg / pos, # Booster Parameter: Uses the ratio of neg/pos to ensure that the values are weighted\n",
    "    eval_metric='logloss', #  Learning Task Parameter: Chooses the “log loss” score for measuring mistakes during training. usually selected for binary:logistic\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2ba559-d6cc-4643-ad0c-fea243615739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the Model\n",
    "base_xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on test set\n",
    "y_pred = base_xgb_model.predict(X_test)\n",
    "y_pred_proba = base_xgb_model.predict_proba(X_test)[:, 1]  # Instead of just 0/1, provides confidence on the probability the model assigns to class 1 for each test example.\n",
    "\n",
    "# Compute & print metrics\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\\n\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred), \"\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59474393-48bf-4f8a-9260-02f2951741e1",
   "metadata": {},
   "source": [
    "#### Step 2: A Randomized Hyperparameter Search on xgb_model, optimising for Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e910c5c-a3f6-4ab7-a3e9-4a9b126178aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 1) Split into subtrain / validation\n",
    "X_sub, X_val, y_sub, y_val = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=0.2,\n",
    "    stratify=y_train,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2) Further downsample sub-training set to 50 K rows for hyperparameter search \n",
    "X_tune, _, y_tune, _ = train_test_split( #_, _: The underscores mean “we don’t care about the leftover rows right now.”\n",
    "    X_sub, y_sub,\n",
    "    train_size=50000,\n",
    "    stratify=y_sub,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 3) Base XGB with imbalance handled\n",
    "hyperparameter_xgb_model = XGBClassifier(\n",
    "    eval_metric='logloss',\n",
    "    scale_pos_weight=(y_train==0).sum() / (y_train==1).sum(),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 4) Precision scorer\n",
    "precision_scorer = make_scorer(precision_score) #Wraps scikit-learn’s precision_score into a form that Grid/Random search can use.\n",
    "\n",
    "# 5) Choosing the hyperparameters\n",
    "param_dist = {\n",
    "    'n_estimators':     [100, 150, 200], # try building forests of 100, 150, or 200 trees\n",
    "    'max_depth':        [3, 5, 7], # allow each tree be shallow (depth 3), medium (5) or a bit deeper (7)\n",
    "    'learning_rate':    [0.01, 0.05, 0.1], # try slow or faster learning rates\n",
    "    'subsample':        [0.8, 1.0], # whether to sample 80% or 100% of rows per tree\n",
    "    'colsample_bytree': [0.8, 1.0] # whether to sample 80% or 100% of columns per tree\n",
    "}\n",
    "\n",
    "# 6) Tuning our hyperparameter_xgb_model thru randomised search (20 samples, 3-fold CV)\n",
    "rand_search = RandomizedSearchCV(\n",
    "    estimator=hyperparameter_xgb_model,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=20,                # Only try 20 random combinations out of the full menu (saves time vs. brute-forcing every combo)\n",
    "    scoring=precision_scorer, # Rank combos by precision\n",
    "    cv=3,                     # For each combo, do a quick 3-way “train/test” split to see how it generalizes.\n",
    "    n_jobs=-1, # Use all CPU cores you can\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# 7) Fit on the smaller tuning set\n",
    "rand_search.fit(X_tune, y_tune) # Goes through 20 random model settings, does 3-fold CV each time, and picks the one with highest precision.\n",
    "print(\"Best precision params:\", rand_search.best_params_) # Tells you which combination \"won\"\n",
    "\n",
    "# 8) Training the winning model on the full sub-train set\n",
    "best_xgb_model = rand_search.best_estimator_ # Uses the \"champion\" model (with its chosen hyperparameters)\n",
    "best_xgb_model.set_params(**rand_search.best_params_)\n",
    "best_xgb_model.fit( # Teaches it on all 80% of the data we kept aside in step 1\n",
    "    X_sub,\n",
    "    y_sub,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# 9) Final evaluation on the \"untouched\" test set\n",
    "y_pred = best_xgb_model.predict(X_test)\n",
    "\n",
    "print(f\"Recall: {recall_score(y_test, y_pred):.4f}\\n\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred), \"\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab61546e-49e9-4b8e-b5c1-8270dcfe8f1d",
   "metadata": {},
   "source": [
    "#### Step 3a: Improve Precision Scores (i.e. Precision ≥ 0.7) using Threshold Calibration on 'best_xgb_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e29a7b-0df1-4b95-b199-5934591e48e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'best_xgb_model' is the final fitted model and X_test, y_test are ready.\n",
    "\n",
    "# Ask the model: “For each test example, what’s the chance it belongs to class 1 (the ‘positive’ class)?”\n",
    "# Predict_proba gives you two numbers per example (chance of 0 vs. chance of 1) and [:,1] picks out the “chance of 1.”\n",
    "y_probability = best_xgb_model.predict_proba(X_test)[:,1] \n",
    "\n",
    "# Building the precision/recall trade‐off: comparing the y_test to those predicted probabilities (probs).\n",
    "# Sweep through many possible cut‐off points (i.e. “thresholds”) — from “call it 1 if prob≥0.0” up to “prob≥1.0” — and at each cut-off we record how precise we’d be (of everything we call 1, how many really are 1) and how complete we’d be (of all the actual 1s, how many we catch).\n",
    "# That gives us three lists of numbers: the resulting precision, recall, and the thresholds that produced them.\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_probability)\n",
    "\n",
    "# Identify smallest threshold that gives precision ≥ 0.70\n",
    "idx = np.where(precision >= 0.70)[0] # Look through our precision list and find every position where precision is at least 70%. np.where(...) returns all the spots that meet the rule. [0] just pulls out the raw list of those positions.\n",
    "if len(idx): # “Do we have any thresholds at all that yield ≥70% precision?” If our list of good spots (idx) isn’t empty, go inside the if; otherwise skip to the else.\n",
    "    thr = thresholds[idx[0]] # Pick the very first (smallest) threshold from that list—i.e. the easiest cut-off that already gets us to 70% precision.\n",
    "    print(f\"Use threshold = {thr:.4f} to get precision ≥ 0.7\")\n",
    "    y_pred = (y_probability >= thr).astype(int) # Classify using that new cut-off: If the model’s chance ≥ our chosen threshold, call it 1; otherwise call it 0. .astype(int) just turns those True/False answers into 1/0. \n",
    "    print(f\"Recall: {recall_score(y_test, y_pred):.4f}\\n\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred), \"\\n\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, digits=4))    \n",
    "else:\n",
    "    print(\"Cannot reach precision ≥ 0.7 by thresholding alone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5fdcf8-1048-4b70-bec3-6c579c4e497a",
   "metadata": {},
   "source": [
    "#### Step 3b: Improve F₁ Scores (i.e. F₁ ≥ 0.7) using Threshold Calibration on 'best_xgb_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4055dfcf-c408-47ae-a92d-c9ecb285da86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Get your model’s positive-class probabilities\n",
    "y_probability = best_xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# 2) Compute precision, recall and thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_probability)\n",
    "\n",
    "# 3) Compute F1 at each threshold\n",
    "f1_scores = 2 * precision * recall / (precision + recall + 1e-8)\n",
    "\n",
    "# 4) Option I: pick the threshold that maximises F1\n",
    "best_idx = np.argmax(f1_scores)\n",
    "best_thr = thresholds[best_idx]\n",
    "print(f\"Max F1 = {f1_scores[best_idx]:.4f} at threshold = {best_thr:.4f}\")\n",
    "\n",
    "# 5) Option II: if you *must* hit ≥0.7, find the first threshold that does\n",
    "idxs = np.where(f1_scores >= 0.7)[0]\n",
    "if len(idxs):\n",
    "    thr_07 = thresholds[idxs[0]]\n",
    "    print(f\"Threshold {thr_07:.4f} yields F1 = {f1_scores[idxs[0]]:.4f}\")\n",
    "    use_thr = thr_07\n",
    "else:\n",
    "    print(\"No threshold yields F1 ≥ 0.7; using max‐F1 threshold instead.\")\n",
    "    use_thr = best_thr\n",
    "\n",
    "# 6) Apply threshold for Option II (i.e. F₁ = 0.7)\n",
    "y_pred_threshold = (y_probability >= use_thr).astype(int)\n",
    "\n",
    "# 7) Print full classification report\n",
    "\n",
    "print(f\"Recall: {recall_score(y_test, y_pred_threshold):.4f}\\n\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_threshold), \"\\n\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_threshold, digits=4))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c781df38-9a7b-49bf-aab6-74474f727844",
   "metadata": {},
   "source": [
    "#### Plotting the Precision-Recall Curve for 'best_xgb_model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0a2662-52f5-4de4-a706-3be804b3562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the last recall/precision (they pair with no threshold)\n",
    "recall_pts = recall[:-1]\n",
    "precision_pts = precision[:-1]\n",
    "thresholds_pts = thresholds\n",
    "\n",
    "# Make an interactive line+marker plot, with threshold on hover\n",
    "fig = px.line(\n",
    "    x=recall_pts,\n",
    "    y=precision_pts,\n",
    "    markers=True,\n",
    "    labels={'x':'Recall', 'y':'Precision'},\n",
    "    hover_data={'threshold': thresholds_pts}\n",
    ")\n",
    "fig.update_layout(title='Precision-Recall Curve for best_xgb_model')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf229873-d282-444a-a65a-1358608525fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Determine Information Value (IV) & Weight of Evidence (WOE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a49353-1ae2-4f48-8f05-148be8a1b3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_woe_iv(df, feature, target):\n",
    "    \"\"\"\n",
    "    Calculate Weight of Evidence (WOE) and Information Value (IV) for one feature.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        Your dataset.\n",
    "    feature : str\n",
    "        The name of the feature/column to analyze.\n",
    "    target : str\n",
    "        The name of the binary target column (0/1).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    iv : float\n",
    "        Information Value for the feature.\n",
    "    woe_df : pandas.DataFrame\n",
    "        Per-bin table with counts, distributions, WOE, and IV contribution.\n",
    "    \"\"\"\n",
    "    # 1) Build the bin table\n",
    "    lst = []\n",
    "    totals = df.groupby(feature)[target].count()\n",
    "    events = df.groupby(feature)[target].sum()\n",
    "    non_events = totals - events\n",
    "    total_events = events.sum()\n",
    "    total_non_events = non_events.sum()\n",
    "    \n",
    "    for val in totals.index:\n",
    "        evt = events.loc[val]\n",
    "        non_evt = non_events.loc[val]\n",
    "        dist_evt = evt / total_events\n",
    "        dist_non_evt = non_evt / total_non_events\n",
    "        \n",
    "        # avoid division by zero / log issues\n",
    "        woe = np.log((dist_evt + 1e-8) / (dist_non_evt + 1e-8))\n",
    "        iv_bin = (dist_evt - dist_non_evt) * woe\n",
    "        \n",
    "        lst.append({\n",
    "            feature: val,\n",
    "            'count': totals.loc[val],\n",
    "            'events': evt,\n",
    "            'non_events': non_evt,\n",
    "            'dist_events': dist_evt,\n",
    "            'dist_non_events': dist_non_evt,\n",
    "            'WOE': woe,\n",
    "            'IV_contribution': iv_bin\n",
    "        })\n",
    "    \n",
    "    woe_df = pd.DataFrame(lst).sort_values('WOE')\n",
    "    iv = woe_df['IV_contribution'].sum()\n",
    "    \n",
    "    return iv, woe_df\n",
    "\n",
    "# Example: calculate for one categorical feature\n",
    "iv_home_ownership, woe_home_ownership = calc_woe_iv(\n",
    "    modelling_loan_individual_filled_df, \n",
    "    feature='home_ownership', \n",
    "    target='bad_loan'\n",
    ")\n",
    "print(f\"IV for home_ownership: {iv_home_ownership:.4f}\")\n",
    "display(woe_home_ownership)\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# If you want IV for *all* categorical features:\n",
    "# cat_feats = ['home_ownership', 'verification_status', 'purpose', ...]  # fill in your list\n",
    "\n",
    "# iv_dict = {}\n",
    "# for feat in cat_feats:\n",
    "#     iv, _ = calc_woe_iv(lending_club_loan_clean_individual, feat, 'default')\n",
    "#     iv_dict[feat] = iv\n",
    "\n",
    "# # Sort features by IV\n",
    "# iv_series = pd.Series(iv_dict).sort_values(ascending=False)\n",
    "# print(\"Features ranked by IV:\")\n",
    "# print(iv_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578d3768-aa57-419d-83a8-8a6c031a5deb",
   "metadata": {},
   "source": [
    "## Exporting to .csv for Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b42b632-155b-40e8-91a9-cf7ee39239b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_loan_individual_filled_df = modelling_loan_individual_filled_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd262fc8-5c2c-4142-a88e-4d8571c7a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_loan_individual_filled_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e95d59b-ae45-4715-b1e3-451ef71a4ea4",
   "metadata": {},
   "source": [
    "### Dropping Columns that I will not be using in PowerBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970d7603-dec9-49ee-834f-c380e350fc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop_for_dashboard = ['out_prncp','out_prncp_inv','total_pymnt_inv', #'total_pymnt',\n",
    "                'total_rec_prncp','total_rec_int','total_rec_late_fee','recoveries','collection_recovery_fee','last_pymnt_amnt',\n",
    "                'collections_12_mths_ex_med', \n",
    "                'acc_now_delinq',\n",
    "                'tot_coll_amt','tot_cur_bal','acc_open_past_24mths','avg_cur_bal','bc_open_to_buy','bc_util',\n",
    "                'chargeoff_within_12_mths', 'delinq_amnt',\n",
    "                'mo_sin_old_il_acct','mo_sin_old_rev_tl_op','mo_sin_rcnt_rev_tl_op','mo_sin_rcnt_tl','mort_acc','mths_since_recent_bc',\n",
    "                'num_accts_ever_120_pd','num_actv_bc_tl','num_actv_rev_tl','num_bc_sats','num_bc_tl','num_il_tl','num_op_rev_tl',\n",
    "                'num_rev_accts','num_rev_tl_bal_gt_0','num_sats',\n",
    "                'num_tl_120dpd_2m', 'num_tl_30dpd',\n",
    "                'num_tl_90g_dpd_24m','num_tl_op_past_12m','pct_tl_nvr_dlq','percent_bc_gt_75','pub_rec_bankruptcies','tax_liens','tot_hi_cred_lim',\n",
    "                'total_bal_ex_mort','total_bc_limit','total_il_high_credit_limit',\n",
    "\n",
    "                'updated_grade',\n",
    "\n",
    "                'purpose_others',\t'purpose_renewable_energy',\t'purpose_small_business',\n",
    "                'home_ownership_MORTGAGE', 'home_ownership_NONE', 'home_ownership_OTHER', 'home_ownership_OWN',\t'home_ownership_RENT',\n",
    "\n",
    "                'addr_state_AL','addr_state_AR','addr_state_AZ','addr_state_CA','addr_state_CO','addr_state_CT',\n",
    "                'addr_state_DC','addr_state_DE','addr_state_FL','addr_state_GA','addr_state_HI','addr_state_IA', \n",
    "                'addr_state_ID', 'addr_state_IL', 'addr_state_IN', 'addr_state_KS', 'addr_state_KY', 'addr_state_LA', \n",
    "                'addr_state_MA', 'addr_state_MD', 'addr_state_ME', 'addr_state_MI', 'addr_state_MN', 'addr_state_MO', \n",
    "                'addr_state_MS', 'addr_state_MT', 'addr_state_NC', 'addr_state_ND', 'addr_state_NE', 'addr_state_NH', \n",
    "                'addr_state_NJ', 'addr_state_NM', 'addr_state_NV', 'addr_state_NY', 'addr_state_OH', 'addr_state_OK', \n",
    "                'addr_state_OR', 'addr_state_PA', 'addr_state_RI', 'addr_state_SC', 'addr_state_SD', 'addr_state_TN', \n",
    "                'addr_state_TX', 'addr_state_UT', 'addr_state_VA', 'addr_state_VT', 'addr_state_WA', 'addr_state_WI', \n",
    "                'addr_state_WV', 'addr_state_WY'\n",
    "                  ]\n",
    "\n",
    "dashboard_loan_individual_filled_df = dashboard_loan_individual_filled_df.drop(cols_to_drop_for_dashboard, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3181e3-66a7-4fa9-a4d2-d474e86f9a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_loan_individual_filled_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982dd232-99eb-452d-a6e0-cb1cf9dd4971",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard_loan_individual_filled_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c87e4e8-46fd-4220-8ee8-42d3e2433a0a",
   "metadata": {},
   "source": [
    "### Downsampling my Dataset for ease of Import into PowerBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0d7514-57df-455c-8933-8bc6761dcd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determining sample size\n",
    "sample_size = 0.30\n",
    "\n",
    "loan_sampled, loan_remainder = train_test_split(\n",
    "    dashboard_loan_individual_filled_df,\n",
    "    train_size=sample_size,\n",
    "    stratify=dashboard_loan_individual_filled_df['bad_loan'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Check on whether ratio in loan_sampled = dashboard_loan_individual_filled_df\n",
    "print(\"Original ratios:\\n\", dashboard_loan_individual_filled_df['bad_loan'].value_counts(normalize=True))\n",
    "print(\"Sampled ratios:\\n\", loan_sampled['bad_loan'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c6bb41-608b-4011-aff6-2a8d81cdb3a4",
   "metadata": {},
   "source": [
    "### Exporting the sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7b1589-8f65-4a57-81ca-37dfa5ca053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_sampled.to_csv('./dashboard_loan_individual_filled_df.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
